
import os
import json
import argparse
import requests
from typing import List, Dict
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import google.generativeai as genai

# --- Configuration and Initialization ---

def configure_api():
    """
    Loads environment variables and configures the Generative AI API.
    Expects the API key in a .env file.
    """
    load_dotenv(override=True)
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        print("Error: GEMINI_API_KEY not found. Please create a .env file and add your API key.")
        exit(1)
    genai.configure(api_key=api_key)
    print("Gemini API configured successfully.")

# --- Web Scraping Class ---

class Website:
    """
    A class to represent and scrape a single webpage.
    It fetches the content, extracts the title, clean text, and all links.
    """
    # Standard headers to mimic a browser visit
    HEADERS = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36"
    }

    def __init__(self, url: str):
        """
        Initializes a Website object by scraping the given URL.

        Args:
            url (str): The URL of the website to scrape.
        """
        self.url = url
        self.title = "No title found"
        self.text = ""
        self.links = []
        
        try:
            response = requests.get(url, headers=self.HEADERS, timeout=10)
            response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)

            soup = BeautifulSoup(response.content, 'html.parser')
            
            self.title = soup.title.string if soup.title else "No title found"
            
            # Decompose irrelevant tags to clean up the text
            if soup.body:
                for irrelevant in soup.body(["script", "style", "img", "input", "nav", "footer", "header"]):
                    irrelevant.decompose()
                self.text = soup.body.get_text(separator="\n", strip=True)
            else:
                self.text = "" # Fallback for pages without a body
                
            # Extract and filter links
            page_links = [link.get('href') for link in soup.find_all('a')]
            self.links = [link for link in page_links if link and not link.startswith(('#', 'mailto:', 'tel:'))]

        except requests.exceptions.RequestException as e:
            print(f"Error fetching website {self.url}: {e}")
            # Allow the process to continue with empty content

    def get_contents(self) -> str:
        """
        Returns a formatted string of the webpage's title and text content.
        """
        return f"Webpage Title: {self.title}\nWebpage Contents:\n{self.text}\n\n"

# --- AI-Powered Functions ---

def get_relevant_links(website: Website) -> List[Dict[str, str]]:
    """
    Uses the Gemini AI to analyze links from a website and identify relevant ones
    for a company brochure (e.g., about, careers, solutions).

    Args:
        website (Website): A scraped Website object.

    Returns:
        A list of dictionaries, where each dictionary represents a relevant link
        with its type and full URL.
    """
    print(f"AI is analyzing links from {website.url}...")
    
    # System instruction for the model
    system_prompt = (
        "You are an expert assistant that analyzes a list of website links. "
        "Your task is to identify which links are most relevant for building a company brochure. "
        "Focus on pages like 'About Us', 'Company', 'Solutions', 'Products', or 'Careers/Jobs'. "
        "Convert all relative links (e.g., '/about') to absolute URLs using the base URL. "
        "Ignore links for 'Terms of Service', 'Privacy Policy', social media, or login pages. "
        "You must respond ONLY with a JSON object in the following format:\n"
        '{"links": [{"type": "page type", "url": "https://full.url/goes/here"}, ...]}'
    )

    # User prompt providing the context and the list of links
    user_prompt = (
        f"Base URL: {website.url}\n\n"
        f"Here is the list of links from the website. Decide which are relevant for a brochure "
        f"and provide their full, absolute URLs in the specified JSON format.\n\n"
        f"Links:\n" + "\n".join(website.links)
    )

    try:
        model = genai.GenerativeModel(
            model_name='gemini-1.5-flash',
            generation_config={"response_mime_type": "application/json"}
        )
        response = model.generate_content([system_prompt, user_prompt])
        
        # The response text should be a valid JSON string
        result_json = json.loads(response.text)
        print("AI analysis complete. Found relevant links.")
        return result_json.get("links", [])
    except Exception as e:
        print(f"An error occurred during AI link analysis: {e}")
        return []


def get_all_details(url: str) -> str:
    """
    Gathers all textual content from the main page and other relevant pages.

    Args:
        url (str): The base URL of the company.

    Returns:
        A single string containing all collected text.
    """
    print("\nStep 1: Scraping landing page...")
    main_site = Website(url)
    all_text = f"== START: Content from Landing Page ({url}) ==\n"
    all_text += main_site.get_contents()
    all_text += "== END: Content from Landing Page ==\n\n"

    print("\nStep 2: Finding and scraping relevant sub-pages...")
    relevant_links = get_relevant_links(main_site)

    if not relevant_links:
        print("No relevant sub-pages found by AI. Proceeding with landing page content only.")
        return all_text

    for link_info in relevant_links:
        link_url = link_info.get("url")
        link_type = link_info.get("type", "page")
        if link_url:
            print(f"Scraping '{link_type}' page: {link_url}")
            page_content = Website(link_url).get_contents()
            all_text += f"== START: Content from {link_type.title()} Page ({link_url}) ==\n"
            all_text += page_content
            all_text += f"== END: Content from {link_type.title()} Page ==\n\n"

    return all_text


def stream_brochure(company_name: str, all_text: str, tone: str = "professional"):
    """
    Generates and streams a company brochure using the collected text.

    Args:
        company_name (str): The name of the company.
        all_text (str): The compiled text from all scraped pages.
        tone (str): The desired tone of the brochure (e.g., 'professional', 'humorous').
    """
    print("\nStep 3: Generating brochure... (AI is writing)\n---")

    # Define the system prompt based on the desired tone
    system_prompt = (
        f"You are an expert marketing assistant. Your task is to write a compelling, {tone} company brochure "
        "in Markdown format for prospective customers, investors, and recruits. "
        "Use the provided text, which was scraped from the company's website. "
        "Structure the brochure logically with clear headings. If available, highlight information "
        "about company culture, key products/solutions, and career opportunities."
    )

    # User prompt with the company context and the collected text
    user_prompt = (
        f"Company Name: {company_name}\n\n"
        "Here is the collected content from the company's website. Use this to create the brochure.\n\n"
        f"--- WEBSITE CONTENT ---\n{all_text}"
    )

    try:
        model = genai.GenerativeModel(model_name='gemini-1.5-flash')
        stream = model.generate_content(
            [system_prompt, user_prompt],
            stream=True
        )

        for chunk in stream:
            # Print the content directly to the console as it arrives
            print(chunk.text, end="", flush=True)

        print("\n---\nBrochure generation complete.")

    except Exception as e:
        print(f"\nAn error occurred during brochure generation: {e}")


# --- Main Execution ---

def main():
    """
    Main function to run the brochure generator from the command line.
    """
    parser = argparse.ArgumentParser(description="AI-Powered Company Brochure Generator")
    parser.add_argument("company_name", type=str, help="The name of the company.")
    parser.add_argument("url", type=str, help="The full URL of the company's homepage (e.g., https://example.com).")
    parser.add_argument(
        "--tone", 
        type=str, 
        default="professional", 
        choices=["professional", "humorous", "inspirational", "technical"],
        help="The tone of the brochure."
    )
    args = parser.parse_args()

    # Validate URL format
    if not args.url.startswith(('http://', 'https://')):
        print("Error: Please provide a full URL including http:// or https://")
        return

    print(f"Starting brochure creation for '{args.company_name}' from {args.url}")
    
    # Set up the API
    configure_api()

    # Gather all website content
    all_text = get_all_details(args.url)

    # Generate and stream the final brochure
    stream_brochure(args.company_name, all_text, args.tone)

if __name__ == "__main__":
    main()
